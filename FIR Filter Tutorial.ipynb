{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Python 3.7 Intoduction to FIR Filtering</h1>\n",
    "<h5>By: Erica Sawczynec</h5>\n",
    "\n",
    "<p>The goal of this tutorial is to walk you through how I simulate the results of 3 bit FIR filtered data in order to familiarize you with our data set, FIR filtering in Python 3.7, how we can estimate the pulse height and time after filtering, and finally, reproduce the filtering results I found.</p>\n",
    "\n",
    "<p>Let's start with a little bit about our data set.  The data I have provided to you was taken using two SensL SiPM's read out to two different channels on a Infinium V-Series oscilloscope. Each SiPM fed a ~30 V from a Mastech DC power supply.  For this particular dataset the events are produced from a CS-137 source placed closely to the two SiPMs. Each SiPM is attached to a different channel, in the dataset files you will notice they are labeled Channel 3 and Channel 4.  Each channel has the same sampling rate and voltage trigger on the pulse. If you are not familiar with oscilloscopes, triggers are set in either voltage or time so the scope knows when to record events.  In our case, I programmed the scope so the incoming event voltage has to be above 3 mV on both channels otherwise the event will not be recorded.</p>\n",
    "\n",
    "<img src=\"img/SiPM_example.jpeg\" width=\"400\" height=\"200\">\n",
    "\n",
    "<div align=\"center\">An example of a recorded SiPM event on the scope.  Channel 3 is shown in blue and Channel 4 is shown in red.  Notice the voltage difference between the two channels for the same event, this will be discussed later in this tutorial.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we import all the packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy import interpolate\n",
    "import numpy.polynomial.polynomial as poly\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After importing all the packages we want to use the first thing we want to do is read in the data files.  Each file has a column for time and voltage.  I prefer using pandas to turn each event into a dataframe and then store each dataframe in a dictionary where the number of the event is the key.  If you are not familiar with dictionaries in python <a href=\"https://www.w3schools.com/python/python_dictionaries.asp\">this</a> is a short walk through of the basics.</p>\n",
    "\n",
    "<p>In the following cell I have removed the code I would usually put into pd.read_csv() function.  <a href= \"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\">Here</a> is the documentation for that function. See if you can read in the events writing your own code here.  The two lines below the pd.read_csv() function should give you an indication of what the columns for each event should be named.  I multiply the times by 10**9 to have them in ns and I multiply our Voltages by 10**3 to have them in mV.  I have also removed the number of events to iterate through so you can select that.  Remember that reading in all the events will take time, so it might be beneficial to iterate through the first 10 events to see if you code works.  Here are some other links that might help: <a href = \"https://realpython.com/python-f-strings/\">formatting f strings</a>, <a href = \"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\">pd.Dataframe</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an ordered dictionary is a little different from a normal dictionary, but they have the same function\n",
    "#it's easy to seperate the data into each channel and save it that way\n",
    "data_ch3, data_ch4 = (OrderedDict() in range(2))\n",
    "\n",
    "#I prefer starting my keys with 0 instead of 1 (to follow python indexing rules)\n",
    "a = 0\n",
    "#now we loop through all the events in the dataset\n",
    "for iteration in range(1, ):\n",
    "    #we use if, elif, and else statements because there are varying numbers of 0 in front of each number in the dataset\n",
    "    if iteration < 10:\n",
    "        #read in the event from a channel 3 file\n",
    "        data_ch3[str(a)] = pd.read_csv()\n",
    "        #multiply const so time is in ns\n",
    "        data_ch3[str(a)].times = data_ch3[str(a)].times * 10**9 \n",
    "        #multiply const so voltage is in ns\n",
    "        data_ch3[str(a)].Voltages = data_ch3[str(a)].Voltages * 10**3\n",
    "        #increment a for next event\n",
    "        \n",
    "        #read in the event from a channel 4 file\n",
    "        data_ch4[str(a)] = pd.read_csv()\n",
    "        #multiply const so time is in ns\n",
    "        data_ch4[str(a)].times = data_ch4[str(a)].times * 10**9 \n",
    "        #multiply const so voltage is in ns\n",
    "        data_ch4[str(a)].Voltages = data_ch4[str(a)].Voltages * 10**3\n",
    "        #increment a for next event\n",
    "        \n",
    "        a+=1\n",
    "    elif iteration >= 10 and iteration <= 99:\n",
    "        #read in the event from a channel 3 file\n",
    "        data_ch3[str(a)] = pd.read_csv()\n",
    "        data_ch3[str(a)].times = data_ch3[str(a)].times * 10**9\n",
    "        data_ch3[str(a)].Voltages = data_ch3[str(a)].Voltages * 10**3\n",
    "        \n",
    "        #read in the event from a channel 4 file\n",
    "        data_ch4[str(a)] = pd.read_csv()\n",
    "        data_ch4[str(a)].times = data_ch4[str(a)].times * 10**9 \n",
    "        data_ch4[str(a)].Voltages = data_ch4[str(a)].Voltages * 10**3\n",
    "        \n",
    "        a+=1\n",
    "    elif iteration > 99 and iteration <=999:\n",
    "        #read in the event from a channel 3 file\n",
    "        data_ch3[str(a)] = pd.read_csv()\n",
    "        data_ch3[str(a)].times = data_ch3[str(a)].times * 10**9\n",
    "        data_ch3[str(a)].Voltages = data_ch3[str(a)].Voltages * 10**3\n",
    "        \n",
    "        #read in the event from a channel 4 file\n",
    "        data_ch4[str(a)] = pd.read_csv()\n",
    "        data_ch4[str(a)].times = data_ch4[str(a)].times * 10**9 \n",
    "        data_ch4[str(a)].Voltages = data_ch4[str(a)].Voltages * 10**3\n",
    "        \n",
    "        a+=1\n",
    "    else:\n",
    "        #read in the event from a channel 3 file\n",
    "        data_ch3[str(a)] = pd.read_csv()\n",
    "        data_ch3[str(a)].times = data_ch3[str(a)].times * 10**9\n",
    "        data_ch3[str(a)].Voltages = data_ch3[str(a)].Voltages * 10**3\n",
    "        \n",
    "        #read in the event from a channel 4 file\n",
    "        data_ch4[str(a)] = pd.read_csv()\n",
    "        data_ch4[str(a)].times = data_ch4[str(a)].times * 10**9 \n",
    "        data_ch4[str(a)].Voltages = data_ch4[str(a)].Voltages * 10**3\n",
    "        \n",
    "        a+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We are interested in some particular aspects of each event that we want to save for comparison to our recovery values later.  I show you how to quickly find the maximum voltage for each event, but I've deleted the code that finds the time that the maximum voltage occurs and where the information for each event is saved in one dataframe.  See if you can fill in this code.  Here are some links that may help: <a href = \"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\">indexing</a>, <a href = \"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\">pd.concat</a>, <a href = \"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\">pd.Dataframe</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_ch3, great_ch4 = (pd.DataFrame([]) for i in range(2))\n",
    "great_index, great_times, great_peak = (pd.Series([]) for i in range(3))\n",
    "\n",
    "for key, value in data_ch3.items():\n",
    "    #find the maximum voltage of the event\n",
    "    great_index[int(key)] = data_ch3[str(key)]['Voltages'].idxmax()\n",
    "    #find the time the maximum voltage occurs\n",
    "\n",
    "#save the maximum value of each event, the time the maximum value occurs, and 1/2 the maximum value in a data frame\n",
    "#name each column max_vals, times, and half_max respectively\n",
    "#named great_ch3\n",
    "\n",
    "\n",
    "great_index, great_times, great_peak = (pd.Series([]) for i in range(3))\n",
    "\n",
    "#find the max vals and times the max val occurs for each event\n",
    "for key, value in data_ch4.items():\n",
    "    #find the maximum voltage of the event\n",
    "    great_index[int(key)] = data_ch4[str(key)]['Voltages'].idxmax()\n",
    "    #find the time the maximum voltage occurs\n",
    "\n",
    "#save the maximum value of each event, the time the maximum value occurs, and 1/2 the maximum value in a data frame\n",
    "#name each column max_vals, times, and half_max respectively\n",
    "#named great_ch4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There are some events that we want to remove because they are gross outliers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_index_ch4 = great_ch4[great_ch4['max_vals'] > 50].index\n",
    "\n",
    "for index in bad_index_ch4:\n",
    "    del data_ch4[str(index)]\n",
    "    del data_ch3[str(index)]\n",
    "\n",
    "great_ch3 = great_ch3.drop(great_ch3.index[[bad_index_ch4]])\n",
    "great_ch4 = great_ch4.drop(great_ch4.index[[bad_index_ch4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Unfortunatly the events off of the scope are not sampled at the rate we want them to be.  We are aiming for 1 GSPS sampling, which implies the difference between each sucessive time increment should be 1 ns.  This next cell takes one in every 40 timing points to mimic the sampling rate we want.  I've removed the code that takes one in every 40 points.  See if you can attain the sampling rate we want.  There is a commented print statement at the bottom of the cell that will help you check if there is 1 ns between successive time increments. Here is a link that might help: <a href = \"https://numpy.org/doc/stable/reference/arrays.indexing.html\">numpy indexing</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again we want to save each event dataframe in a dictionary\n",
    "ori_ch3 = OrderedDict()\n",
    "#here we initialize some arrays\n",
    "ori_times, ori_vals = (pd.Series([]) for i in range(2))\n",
    "temp = pd.DataFrame([])\n",
    "\n",
    "#dict.items() is how you can iterate through the dictionary by key and the corresponding key's dataframe (values)\n",
    "for key, values in data_ch3.items():\n",
    "        \n",
    "    #take one in every 40 values for time\n",
    "    ori_times = \n",
    "    #take one in every 40 values for voltages\n",
    "    ori_vals = \n",
    "    \n",
    "    #make a df of times and voltages\n",
    "    #save to the dictionary under that event name\n",
    "    temp['times'] = ori_times\n",
    "    temp['Voltages'] = ori_vals\n",
    "    ori_ch3[key] = temp\n",
    "\n",
    "    #recreate the df so theres no confusion\n",
    "    temp = pd.DataFrame([])\n",
    "    \n",
    "\n",
    "ori_ch4 = OrderedDict()\n",
    "ori_times, ori_vals = (pd.Series([]) for i in range(2))\n",
    "temp = pd.DataFrame([])\n",
    "\n",
    "for key, values in data_ch4.items():\n",
    "    \n",
    "    #take one in every 40 values for time\n",
    "    ori_times = \n",
    "    #take one in every 40 values for voltages\n",
    "    ori_vals = \n",
    "\n",
    "    #make the df of times and voltages\n",
    "    #save to the dictionary under that event name\n",
    "    temp['times'] = ori_times\n",
    "    temp['Voltages'] = ori_vals\n",
    "    ori_ch4[key] = temp\n",
    "\n",
    "    #recreate the df so theres no confusion\n",
    "    temp = pd.DataFrame([])\n",
    "    \n",
    "#let's check if we have the sampling rate we want\n",
    "#print(\"The time between each timing increment is for Channel 3 is ()\".format(ori_ch3[str(0)].times.iloc[1]-ori_ch3[str(0)].times.iloc[0]))\n",
    "#print(\"The time between each timing increment is for Channel 4 is ()\".format(ori_ch4[str(0)].times.iloc[1]-ori_ch4[str(0)].times.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have a dataset with the initial sampling rate we want.  We also want to create two other datasets with 2 GSPS (500 ps sampling) and 1 GSPS (1 ns sampling) for each channel.  This next cell is similar to the last cell in terms of code, so it should be straight forward what needs to be done here.  For the 2 GSPS dataset we want 5 ns between timing increments and for the 1 GSPS dataset we want 10 ns between timing increments.  As the cell above, there are print statements at the bottom of the cell you can use to check if you have the right sampling for each dictionary.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the new dictionaries\n",
    "one_nano_ch3, five_hundred_pico_ch3 = (OrderedDict() for i in range(2))\n",
    "#formality\n",
    "one_nano_times, one_nano_vals, five_hundred_pico_times, five_hundred_pico_vals = (pd.Series([]) for i in range(4))\n",
    "temp,temp1 = (pd.DataFrame([]) for i in range(2))\n",
    "\n",
    "#now we can iterate through the events we just created and downsample from there\n",
    "for key, values in ori_ch3.items():\n",
    "        \n",
    "    #keep the time vals consistant\n",
    "    one_nano_times = \n",
    "    one_nano_vals = \n",
    "\n",
    "    #keep time vals constant\n",
    "    five_hundred_pico_times = \n",
    "    five_hundred_pico_vals = \n",
    "\n",
    "    #make the df of times and voltages\n",
    "    #save to the dictionary under that event name\n",
    "    temp['times'] = one_nano_times\n",
    "    temp['Voltages'] = one_nano_vals\n",
    "    one_nano_ch3[str(key)] = temp\n",
    "    temp1['times'] = five_hundred_pico_times\n",
    "    temp1['Voltages'] = five_hundred_pico_vals\n",
    "    five_hundred_pico_ch3[str(key)] = temp1\n",
    "\n",
    "    #recreate the df so theres no confusion\n",
    "    temp = pd.DataFrame([])\n",
    "    temp1 = pd.DataFrame([])\n",
    "    \n",
    "#this resamples to create the 1ns and 500 ps events for the training data set\n",
    "#initialize\n",
    "one_nano_ch4, five_hundred_pico_ch4 = (OrderedDict() for i in range(2))\n",
    "one_nano_times, one_nano_vals, five_hundred_pico_times, five_hundred_pico_vals = (pd.Series([]) for i in range(4))\n",
    "temp,temp1 = (pd.DataFrame([]) for i in range(2))\n",
    "\n",
    "for key, values in ori_ch4.items():\n",
    "        \n",
    "    one_nano_times = \n",
    "    one_nano_vals = \n",
    "\n",
    "    five_hundred_pico_times = \n",
    "    five_hundred_pico_vals = \n",
    "\n",
    "    #make the df of times and voltages\n",
    "    #save to the dictionary under that event name\n",
    "    temp['times'] = one_nano_times\n",
    "    temp['Voltages'] = one_nano_vals\n",
    "    one_nano_ch4[str(key)] = temp\n",
    "    temp1['times'] = five_hundred_pico_times\n",
    "    temp1['Voltages'] = five_hundred_pico_vals\n",
    "    five_hundred_pico_ch4[str(key)] = temp1\n",
    "\n",
    "    #recreate the df so theres no confusion\n",
    "    temp = pd.DataFrame([])\n",
    "    temp1 = pd.DataFrame([])\n",
    "    \n",
    "#let's check if we have the sampling rate we want\n",
    "#print(\"The time between each timing increment for the 500 ps Channel 3 data is ()\".format(five_hundred_pico_ch3[str(0)].times.iloc[1]-five_hundred_pico_ch3[str(0)].times.iloc[0]))\n",
    "#print(\"The time between each timing increment for the 500 ps Channel 4 data is ()\".format(five_hundred_pico_ch4[str(0)].times.iloc[1]-five_hundred_pico_ch4[str(0)].times.iloc[0]))\n",
    "#print(\"The time between each timing increment for the 1 ns Channel 3 data is ()\".format(one_nano_ch3[str(0)].times.iloc[1]-one_nano_ch3[str(0)].times.iloc[0]))\n",
    "#print(\"The time between each timing increment for the 1 ns Channel 4 data is ()\".format(one_nano_ch4[str(0)].times.iloc[1]-one_nano_ch4[str(0)].times.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now all the data is downsampled in time like we want it to be, but in order to simulate 3-bit data we also need to downsample in voltage.  For a 3-bit system we can have (2**3)-1 comparator thresholds in the FPGA.  After playing with the data, I've found the most useful comparator thresholds are generated when using the half_max values we saved in the great dataframes above.  Here is a formula for creating the increment of the threshold:</p>\n",
    "\n",
    "<p>increment = ((maximum + 1) - (minimum - 1)) / (number of thresholds - 1)</p>\n",
    "\n",
    "<p>Where maximum is the largest half_max value in the great df and minimum is the smallest half_max value in the great df.</p>\n",
    "\n",
    "<p>See if you can build a fuction that generates the 6 downsampling thresholds above the noise threshold. Here is a hint: the first downsampling threshold should be initial + increment (given by the equation above).</p>\n",
    "\n",
    "<p>Also note that here we only use great_ch4 because on average the events are larger in that data set.  All the events from channel 3 should be encompassed by the downsampling thresholds anyway.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------this creates and saves the downsampled thresholds -------------\n",
    "\n",
    "#this creates the downsampled thresholds based on the values of the largest and smallest event peaks\n",
    "#currently simulates a 3 bit system\n",
    "def downsample(great, num, initial):\n",
    "\n",
    "    return downsampled_thres\n",
    "\n",
    "downsampled_thres = []\n",
    "#num = number of downsampling thresholds\n",
    "num = 6\n",
    "#initial = noise threshold\n",
    "initial = 2\n",
    "downsampled_thres = downsample(great_ch4, num, initial)\n",
    "\n",
    "print(\"The downsampling thresholds are: {}\".format(downsampled_thres))\n",
    "\n",
    "#------this is the end of the stuff that creates and saves the downsampled thresholds -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I will give you the following functions for free because we want to downsample the events in voltage in a particular way.  I think the following image describes it better than I could in words:</p>\n",
    "<img src=\"files/downsampling_w_thresholds.png\" width=\"800\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "thresholded takes the original finely sampled data and downsamples it in voltage based on the\n",
    "thresholds determined in the cell above\n",
    "\n",
    "takes: some form of data dictionary, the downsampling in voltages thresholds, the key of the event, i (amount of downsampling in voltage thresholds + 1)\n",
    "returns: the downsampled in voltage pulse for that particular event\n",
    "\"\"\"\n",
    "def thresholded(data, downsampled_thres, key, i):\n",
    "    #we just want the voltages of the pulse not the time (the times are already downsampled at this point)\n",
    "    pulse = np.array(data[str(key)]['Voltages'])\n",
    "    for ii in range(0, i+1):\n",
    "        #if the voltage value is less than the noise threshold we want the downsampled in voltage pulse to be equal to 0\n",
    "        if ii == 0:\n",
    "            pulse[:] = [x if x >= downsampled_thres[ii] else 0 for x in pulse]\n",
    "        #if the voltage value is equal to zero (from above) or greater than the second downsample threshold make no change\n",
    "        #otherwise set the voltage value equal to the noise threshold\n",
    "        if ii == 1:\n",
    "            pulse[:] = [x if (x == 0 or x > downsampled_thres[ii]) else downsampled_thres[ii - 1] for x in pulse]\n",
    "        #if we are anywhere inbetween the 2nd threshold and the last thresholded\n",
    "        #if the voltage is less than the downsampled threshold just implemented and greater than the final threshold, make no change\n",
    "        #otherwise set the voltages between that range to the downsampled threshold\n",
    "        if ii != i and ii != 1 and ii != 0 and ii != len(downsampled_thres):\n",
    "            pulse[:] = [x if (x < downsampled_thres[ii-1] or x >= downsampled_thres[ii]) else downsampled_thres[ii-1] for x in pulse]\n",
    "        #finally if the values are less then the last downsampled thresold dont make any changes\n",
    "        #otherwise set the values greater than the last downsampled threshold to the value of the last downsampled threshold\n",
    "        if ii == i:\n",
    "            pulse[:] = [x if x < downsampled_thres[len(downsampled_thres)-1] else downsampled_thres[len(downsampled_thres) - 1] for x in pulse]\n",
    "\n",
    "    return pulse\n",
    "\n",
    "\"\"\"\n",
    "thres2df concats the downsampled in time times for the pulse and the downsampled in voltage pulse and returns it as a dataframe\n",
    "\n",
    "takes: some form of data dictionary, the downsamping in voltage thresholds, the key of the event, i (the amount of downsamping in voltage thresholds)\n",
    "returns: a downsampled in time and voltge pulse dataframe for the particular event\n",
    "\"\"\"\n",
    "def thres2df(diction, downsampled_thres, key, i):\n",
    "    #initialize the df\n",
    "    temp1 = pd.DataFrame([])\n",
    "\n",
    "    #create the downsampled in voltage pulse using thresholded\n",
    "    temp = thresholded(diction, downsampled_thres, key, i)\n",
    "    #fill the initialized dataframe with the returned voltages\n",
    "    temp1 = pd.DataFrame(temp)\n",
    "    #concat the downsampled in time and downsampled in voltage dfs\n",
    "    temp1 = pd.concat([diction[str(key)]['times'].rename('times'), temp1], axis = 1)\n",
    "    #set the names of the columns so it's easier to navigate later\n",
    "    temp1.columns = ['times', 'Voltages']\n",
    "\n",
    "    return temp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Use the above functions to create the downsampled in voltage and time events.  This is basically just a straight forward function call.  If you don't understand what any of the above code is doing, feel free to shoot me an email and ask, just make sure you include the lines you are confused about!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----this saves all the data downsampled in time and voltage in respective dictionaries ------------\n",
    "\n",
    "#downsample the data and find the max downsampling threshold it crosses for the calibration data\n",
    "thresholded_data_ch3, thresholded_data_nano_ch3, thresholded_data_pico_ch3 = (OrderedDict() for i in range(3))\n",
    "thresholded_data_ch4, thresholded_data_nano_ch4, thresholded_data_pico_ch4 = (OrderedDict() for i in range(3))\n",
    "\n",
    "i = len(downsampled_thres)\n",
    "\n",
    "for key, value in ori_ch3.items():\n",
    "\n",
    "    #downsample original data in voltage + save in dict\n",
    "    thresholded_data_ch3[str(key)] = \n",
    "    thresholded_data_ch4[str(key)] = \n",
    "\n",
    "    #downsample 500ps data in voltage and save in dict\n",
    "    thresholded_data_pico_ch3[str(key)] = \n",
    "    thresholded_data_pico_ch4[str(key)] = \n",
    "\n",
    "    #downsample 1 ns in voltage + save in dict\n",
    "    thresholded_data_nano_ch3[str(key)] = \n",
    "    thresholded_data_nano_ch4[str(key)] = \n",
    "\n",
    "#-----this is the end of saving all the data downsampled in time and voltage it in respective dictionaries ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we want to create our filter.  The following code is something that Kurtis wrote the bulk of and I manipluated in order to make it do what I wanted it to do.  The idea here is that a SiPM event can be easily simulated by two exponential equations with different taus. A small tau for the leading edge (because the rise of the event is fast) and a larger tau for the falling edge (because the fall of the event is slow).  If you haven't plotted one of the SiPM events yet, I would highly recommend trying it so you can understand what I mean. </p>\n",
    "\n",
    "<p>We want our filter to mimic our events closely, so I have picked the following tau's because they fit our data best.  However, if you want to see how changing those numbers changes the results of filtering feel free to go back and play with this part of the tutorial after running through it once!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----this is where we generate the filter template for channel 3------------------\n",
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Time of the pulse peak\n",
    "pulseTime = 0\n",
    "\n",
    "# Time constants in nanoseconds\n",
    "leadingEdgeTau = 2.0\n",
    "fallingEdgeTau = 20.0\n",
    "\n",
    "# Noise level for Gaussian noise in mV\n",
    "noiseSigma = 0\n",
    "\n",
    "# Sampling rate in ns (GHz)\n",
    "sampleRate = 1.0001\n",
    "sampleStep = 1 / sampleRate\n",
    "# Time to start and stop waveform record\n",
    "startTime = ori_ch3[str(0)].times.iloc[0]\n",
    "stopTime = ori_ch3[str(0)].times.iloc[len(ori_ch3[str(0)])-1]\n",
    "\n",
    "# Generate a random offset for all the pulses\n",
    "pulseTime = pulseTime\n",
    "startTime = startTime\n",
    "stopTime  = stopTime\n",
    "\n",
    "amp = 3\n",
    "\n",
    "sampleData = []\n",
    "sampleTime = []\n",
    "    \n",
    "times = np.arange(startTime, stopTime, sampleStep)\n",
    "\n",
    "# Generate one pulse at time pulseTime\n",
    "for timeNow in times:\n",
    "    sampleTime.append(timeNow);\n",
    "    if timeNow < pulseTime:\n",
    "        #leading edge eq\n",
    "        value = amp * math.exp( (timeNow - pulseTime) / leadingEdgeTau )\n",
    "    else:\n",
    "        #falling edge eq\n",
    "        value = amp * math.exp( - (timeNow - pulseTime) / fallingEdgeTau )\n",
    "    #we aren't adding any noise, but for the sake of clarity this is how we usually add it\n",
    "    value += random.gauss(0,noiseSigma)\n",
    "    sampleData.append(value)\n",
    "    \n",
    "#a simple plot to show the filter compared to the actual data\n",
    "key = 0\n",
    "plt.plot(ori_ch3[str(key)].times, ori_ch3[str(key)].Voltages)\n",
    "plt.plot(times, sampleData)\n",
    "plt.title('Filter Example')\n",
    "plt.xlabel('Time (in ns)')\n",
    "plt.ylabel('Voltage (in mV)')\n",
    "\n",
    "print('Does {} match {}?  If not try changing the sampleRate above.'.format(len(times), len(ori_ch3[str(0)].times)))\n",
    "#-----this is the end of where we generate the filter template for channel 3------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The filter also needs to be downsampled in voltage so it is a 3-bit filter.  Here we do a mini version of what is done to all the events a few cells above.  By the end of this cell we have a 3-bit filter for our channel 3 data set!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the minimum and maximum 'voltage' values\n",
    "maximum = 3\n",
    "minimum = 0\n",
    "\n",
    "#creating the comparator thresholds\n",
    "three_bit = []\n",
    "three_bit.append(minimum)\n",
    "increment = float(maximum-minimum)/4\n",
    "\n",
    "for value in range(0,2):\n",
    "    minimum += increment\n",
    "    three_bit.append(minimum)\n",
    "three_bit.append(maximum)\n",
    "\n",
    "#print the thresholds\n",
    "print(three_bit)\n",
    "\n",
    "i = len(three_bit)\n",
    "\n",
    "#we just want the voltages of the pulse not the time (the times are already downsampled at this point)\n",
    "pulse_ch3 = np.array(sampleData)\n",
    "for ii in range(0, i+1):\n",
    "    #if the voltage value is less than the noise threshold we want the downsampled in voltage pulse to be equal to 0\n",
    "    if ii == 0:\n",
    "        pulse_ch3[:] = [x if x >= three_bit[ii] else 0 for x in pulse_ch3]\n",
    "    #if the voltage value is equal to zero (from above) or greater than the second downsample threshold make no change\n",
    "    #otherwise set the voltage value equal to the noise threshold\n",
    "    if ii == 1:\n",
    "        pulse_ch3[:] = [x if (x == 0 or x > three_bit[ii]) else three_bit[ii - 1] for x in pulse_ch3]\n",
    "    #if we are anywhere inbetween the 2nd threshold and the last thresholded\n",
    "    #if the voltage is less than the downsampled threshold just implemented and greater than the final threshold, make no change\n",
    "    #otherwise set the voltages between that range to the downsampled threshold\n",
    "    if ii != i and ii != 1 and ii != 0 and ii != len(three_bit):\n",
    "        pulse_ch3[:] = [x if (x < three_bit[ii-1] or x >= three_bit[ii]) else three_bit[ii-1] for x in pulse_ch3]\n",
    "    #finally if the values are less then the last downsampled thresold dont make any changes\n",
    "    #otherwise set the values greater than the last downsampled threshold to the value of the last downsampled threshold\n",
    "    if ii == i:\n",
    "        pulse_ch3[:] = [x if x < three_bit[len(three_bit)-1] else three_bit[len(three_bit) - 1] for x in pulse_ch3]\n",
    "        \n",
    "#plot the 3-bit filter!    \n",
    "plt.plot(times, pulse_ch3)\n",
    "plt.xlabel('Time (in ns)')\n",
    "plt.ylabel('Voltages (in mV)')\n",
    "plt.title('Filter Example')\n",
    "plt.show()\n",
    "    \n",
    "#also remember we need to create 3 bit filters for both 2 GSPS and the 1 GSPS data too, otherwise our response will be wrong\n",
    "average_normalized_pico_ch3 = pulse_ch3[0:len(pulse_ch3):5]\n",
    "average_normalized_nano_ch3 = pulse_ch3[0:len(pulse_ch3):10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we walk through the exact same process to create the 3-bit filter for channel 4.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----this is where we generate the filter template------------------\n",
    "# Time of the pulse peak\n",
    "pulseTime = 0\n",
    "\n",
    "# Time constants in nanoseconds\n",
    "leadingEdgeTau = 2.0\n",
    "fallingEdgeTau = 20.0\n",
    "\n",
    "# Noise level for Gaussian noise in mV\n",
    "noiseSigma = 0\n",
    "\n",
    "# Sampling rate in ns (GHz)\n",
    "sampleRate = 1.0001\n",
    "sampleStep = 1 / sampleRate\n",
    "# Time to start and stop waveform record\n",
    "startTime = ori_ch4[str(0)].times.iloc[0]\n",
    "stopTime = ori_ch4[str(0)].times.iloc[len(ori_ch4[str(0)])-1]\n",
    "\n",
    "# Generate a random offset for all the pulses\n",
    "pulseTime = pulseTime\n",
    "startTime = startTime\n",
    "stopTime  = stopTime\n",
    "\n",
    "amp = 3\n",
    "\n",
    "sampleData = []\n",
    "sampleTime = []\n",
    "    \n",
    "times = np.arange(startTime, stopTime, sampleStep)\n",
    "\n",
    "# Generate one pulse at time pulseTime\n",
    "for timeNow in times:\n",
    "    sampleTime.append(timeNow);\n",
    "    if timeNow < pulseTime:\n",
    "        value = amp * math.exp( (timeNow - pulseTime) / leadingEdgeTau )\n",
    "    else:\n",
    "        value = amp * math.exp( - (timeNow - pulseTime) / fallingEdgeTau )\n",
    "    value += random.gauss(0,noiseSigma)\n",
    "    sampleData.append(value)\n",
    "    \n",
    "key = 0\n",
    "plt.plot(ori_ch4[str(key)].times, ori_ch4[str(key)].Voltages, label = 'event')\n",
    "plt.plot(times, sampleData, label = 'filter')\n",
    "plt.title('Filter Example')\n",
    "plt.xlabel('Time (in ns)')\n",
    "plt.ylabel('Voltage (in mV)')\n",
    "\n",
    "print('Does {} match {}?  If not try changing the sampleRate above.'.format(len(times), len(ori_ch4[str(0)].times)))\n",
    "#-----this is the end of where we generate the filter template------------\n",
    "\n",
    "maximum = 3\n",
    "minimum = 0\n",
    "\n",
    "three_bit = []\n",
    "three_bit.append(minimum)\n",
    "increment = float(maximum-minimum)/4\n",
    "\n",
    "for value in range(0,2):\n",
    "    minimum += increment\n",
    "    three_bit.append(minimum)\n",
    "three_bit.append(maximum)\n",
    "\n",
    "print(three_bit)\n",
    "\n",
    "i = len(three_bit)\n",
    "\n",
    "\n",
    "#we just want the voltages of the pulse not the time (the times are already downsampled at this point)\n",
    "pulse_ch4 = np.array(sampleData)\n",
    "for ii in range(0, i+1):\n",
    "    #if the voltage value is less than the noise threshold we want the downsampled in voltage pulse to be equal to 0\n",
    "    if ii == 0:\n",
    "        pulse_ch4[:] = [x if x >= three_bit[ii] else 0 for x in pulse_ch4]\n",
    "    #if the voltage value is equal to zero (from above) or greater than the second downsample threshold make no change\n",
    "    #otherwise set the voltage value equal to the noise threshold\n",
    "    if ii == 1:\n",
    "        pulse_ch4[:] = [x if (x == 0 or x > three_bit[ii]) else three_bit[ii - 1] for x in pulse_ch4]\n",
    "    #if we are anywhere inbetween the 2nd threshold and the last thresholded\n",
    "    #if the voltage is less than the downsampled threshold just implemented and greater than the final threshold, make no change\n",
    "    #otherwise set the voltages between that range to the downsampled threshold\n",
    "    if ii != i and ii != 1 and ii != 0 and ii != len(three_bit):\n",
    "        pulse_ch4[:] = [x if (x < three_bit[ii-1] or x >= three_bit[ii]) else three_bit[ii-1] for x in pulse_ch4]\n",
    "    #finally if the values are less then the last downsampled thresold dont make any changes\n",
    "    #otherwise set the values greater than the last downsampled threshold to the value of the last downsampled threshold\n",
    "    if ii == i:\n",
    "        pulse_ch4[:] = [x if x < three_bit[len(three_bit)-1] else three_bit[len(three_bit) - 1] for x in pulse_ch4]\n",
    "        \n",
    "plt.plot(times, pulse_ch4, label = '3 bit filter')\n",
    "plt.xlabel('Time (in ns)')\n",
    "plt.ylabel('Voltages (in mV)')\n",
    "plt.title('Filter Example')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "average_normalized_pico_ch4 = pulse_ch4[0:len(pulse_ch4):5]\n",
    "average_normalized_nano_ch4 = pulse_ch4[0:len(pulse_ch4):10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here is the point where a few different things can happen.  The filter that I sent you originally only has 7 points, these filters have more than that.  A filter doesn't have to be the same length as the dataset in order to filter the data successfully.  Kurtis had me try out different length filters to see if any had good response.  Ideally, we would use the filter with the lowest amount of points possible that was still able to generate a a good response (which is the filter I sent you). I will put a few cells at the end of the notebook if you are interested in playing with filters of different lengths and the response they give. For the sake of this tutorial, we will just use a filter that has the same length as each of the events.</p>\n",
    "\n",
    "<p>One of the simplest parts of this algorithm is actually filtering each event.  That is thanks to Scipy which is an amazing package that holds all these wonderous functions that make our lives easier. In particular, I'd like to point out all of the functions Scipy offers for <a href = \"https://docs.scipy.org/doc/scipy/reference/signal.html\">signal processing</a>.  I would recommend taking a look through these.  We are interested in the function signal.correlate().  The docs for this function can be found <a href = \"https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html#scipy.signal.correlate\">here</a>.</p>\n",
    "\n",
    "<p>Below I have the basics outlined for our filtering function; however, I have removed the interior of the function so you can write it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- this is the function that does the filtering------\n",
    "from scipy import signal\n",
    "def filtered(h, pulse):\n",
    "    #first we want to corelate the signals\n",
    "    #remember to divide the response by len(pulse)!\n",
    "\n",
    "    #then we want to generate a new data frame that concats the times from pulse and the response from the correlation\n",
    "\n",
    "    #we will return the new dataframe\n",
    "    return temp\n",
    "#---- this is the end of the function that does the filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we want to use our function to find the filter response for each event.  I've left this written for you because I think building the filtering function is more important then calling the function (which you should already be familar with from the exercise above).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- organizing the filtered data ------------\n",
    "filtered_ori_ch3, filtered_pico_ch3, filtered_nano_ch3 = (OrderedDict() for i in range(3))\n",
    "\n",
    "for key, values in ori_ch3.items():\n",
    "    filtered_ori_ch3[str(key)] = filtered(pulse_ch3, ori_ch3[str(key)])\n",
    "    filtered_pico_ch3[str(key)] = filtered(average_normalized_pico_ch3, thresholded_data_pico_ch3[str(key)])\n",
    "    filtered_nano_ch3[str(key)] = filtered(average_normalized_nano_ch3, thresholded_data_nano_ch3[str(key)])\n",
    "#---- the end of organing the filtered data ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- organizing the filtered data ------------\n",
    "filtered_ori_ch4, filtered_pico_ch4, filtered_nano_ch4 = (OrderedDict() for i in range(3))\n",
    "\n",
    "for key, values in ori_ch4.items():\n",
    "    filtered_ori_ch4[str(key)] = filtered(pulse_ch4, ori_ch4[str(key)])\n",
    "    filtered_pico_ch4[str(key)] = filtered(average_normalized_pico_ch4, thresholded_data_pico_ch4[str(key)])\n",
    "    filtered_nano_ch4[str(key)] = filtered(average_normalized_nano_ch4, thresholded_data_nano_ch4[str(key)])\n",
    "#---- the end of organing the filtered data ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can plot the event response for each sampling rate!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just a fancy plotting package that I like\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#select the number of the event you want to look at\n",
    "key = 4\n",
    "plt.figure(figsize = (15, 15))\n",
    "#this plots the filter response for the 10 GSPS data\n",
    "ax = plt.subplot(311)\n",
    "ax.plot(filtered_ori_ch3[str(key)].times, filtered_ori_ch3[str(key)].Voltages, 'r')\n",
    "ax.set_title('Original Data Response')\n",
    "\n",
    "#the 2 GSPS data\n",
    "ax2 = plt.subplot(312)\n",
    "ax2.plot(filtered_pico_ch3[str(key)].times, filtered_pico_ch3[str(key)].Voltages, color = 'green')\n",
    "ax2.set_title('500ps Downsampled Data Response')\n",
    "ax2.set_ylabel('Voltage (in mV)')\n",
    "\n",
    "#the 1 GSPS data\n",
    "ax3 = plt.subplot(313)\n",
    "ax3.plot(filtered_nano_ch3[str(key)].times, filtered_nano_ch3[str(key)].Voltages, 'b')\n",
    "ax3.set_xlabel('Time (in ns)')\n",
    "ax3.set_title('1ns Downsampled Data Response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the number of the event you want to look at\n",
    "key = 4\n",
    "plt.figure(figsize = (15, 15))\n",
    "#this plots the filter response for the 10 GSPS data\n",
    "ax = plt.subplot(311)\n",
    "ax.plot(filtered_ori_ch4[str(key)].times, filtered_ori_ch4[str(key)].Voltages, 'r')\n",
    "ax.set_title('Original Data Response')\n",
    "\n",
    "#for the 2 GSPS data\n",
    "ax2 = plt.subplot(312)\n",
    "ax2.plot(filtered_pico_ch4[str(key)].times, filtered_pico_ch4[str(key)].Voltages, color = 'green')\n",
    "ax2.set_title('500ps Downsampled Data Response')\n",
    "ax2.set_ylabel('Voltage (in mV)')\n",
    "\n",
    "#for the 1 GSPS data\n",
    "ax3 = plt.subplot(313)\n",
    "ax3.plot(filtered_nano_ch4[str(key)].times, filtered_nano_ch4[str(key)].Voltages, 'b')\n",
    "ax3.set_xlabel('Time (in ns)')\n",
    "ax3.set_title('1ns Downsampled Data Response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have all the data we want, we should sort it in a way that makes it easy to pick out interesting trends.  Usually trends arise based on the downsampling threshold that the event crossed, so I elect to sort the data that way.</p>\n",
    "\n",
    "<p>Another important thing we look at in the following functions are the relationship between the filter response peak and time vs. the actually maximum voltage and time of max voltage for each event.  This is how we calibrate the filter reponse to actually mean something. By creating these fits we get coefficents for a polynomial which we can easily implement in the FPGA (or in a NN) to find the actual parameters for a SiPM event given its filter response.  As with all datasets, it's important to have a larger amount of events so more amplitudes and times can be included, that way we don't have to extrapolate far beyond the limits of our simulated data.</p>\n",
    "\n",
    "<p>Again, I will give you these functions because the organizing of data isn't terribly important for you to be bothered with. However, you should understand what these functions are trying to do. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "findpoly finds the polynomial coefficents so we can estimate the maximum voltage value based on the maximum filter response\n",
    "\n",
    "takes:great['max_vals'], the filter response dict, and the last time of the first event.\n",
    "returns:the polynomial coefficents for a 4th order fit\n",
    "'''\n",
    "def findpoly(y, flit, end):\n",
    "    max_value = pd.Series([])\n",
    "    last = pd.Series([])\n",
    "    a = 0\n",
    "    for event, values in flit.items():\n",
    "        max_value[a] = flit[str(event)]['Voltages'].max()\n",
    "        last[a] = flit[str(event)]['times'].loc[len(flit[str(event)])-1]\n",
    "        a += 1\n",
    "   \n",
    "    temp = pd.DataFrame([max_value.rename('recovered'), y.rename('maximum'), last.rename('last')])\n",
    "    temp = temp.T\n",
    "    temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "    #this is a fail safe so if there are any events that hit the limit of the oscilloscope they are removed\n",
    "    temp = temp.query('maximum < 44')\n",
    "    #this is another fail safe so if there are any akward events that trigger late in time they can be removed\n",
    "    temp = temp[temp['last'] >= end]\n",
    "    \n",
    "    #generate the polynomial\n",
    "    z = np.polyfit(temp['recovered'], temp['maximum'], 4)\n",
    "    #get the polynomial coefficents so we can fit things later\n",
    "    p_peak = np.poly1d(z)\n",
    "    \n",
    "    \n",
    "    return p_peak\n",
    "'''\n",
    "sorter sorts the information we are interested in based on maximum voltage threshold crossed by each event and it only\n",
    "returns the events for the threshold of interest\n",
    "\n",
    "takes: an original threshold_data_dict, the downsampling threshold in question, great, the filter response dict, the polynomial coeffs,\n",
    "and the last time recorded for event 0\n",
    "returns: a pd.Series of maximum values, event indexes, maximum peak response, and estimated peak response using the poly coeffs\n",
    "'''    \n",
    "def sorter(diction, threshold, great, peak, p_peak, end):\n",
    "    y, index, max_value, adj, max_times, adj_time, x = (pd.Series([]) for i in range(7))\n",
    "    a = 0\n",
    "    for event, values in diction.items():\n",
    "        #the data is organized based on threshold\n",
    "        if diction[str(event)]['Voltages'].max() == threshold and peak[str(event)]['Voltages'].max() != 0 and diction[str(event)]['times'].iloc[len(diction[str(event)])-1] >= end - 10:\n",
    "            y[a] = great['max_vals'].loc[int(event)]\n",
    "            index[a] = int(event)\n",
    "            max_value[a] = peak[str(event)]['Voltages'].max()\n",
    "            a += 1\n",
    "            \n",
    "    #try to estimate the voltage peak based on the response peak\n",
    "    if len(max_value) != 0:        \n",
    "        adj = p_peak(max_value)\n",
    "\n",
    "    return y, index, max_value, adj\n",
    "'''\n",
    "organizer utilizes sorter to sort the data in each dictionary based on the highest voltage threshold it crosses\n",
    "it then produces things such as voltage residuals and timing residuals and packages it in a nice dataframe which it returns\n",
    "\n",
    "takes: an original threshold_data_dict, the value of the downsampling threshold, great, the filter response dictionary, the corresponding polynomial coeffs,\n",
    "and the last time recorded for event 0\n",
    "returns: organized dataframe\n",
    "'''\n",
    "\n",
    "def organizer(diction, threshold, great, peak, p_peak, end):\n",
    "\n",
    "    #find all the seperate series\n",
    "    y, index, max_value, adj = sorter(diction, threshold, great, peak, p_peak, end)\n",
    "\n",
    "    #we need to find some residuals to see how well the algoritm worked\n",
    "    residuals_max = np.subtract(y, max_value)\n",
    "    adjusted = pd.Series(adj)\n",
    "    residuals_adj = np.subtract(y, adj)\n",
    "    \n",
    "    #concat all the series into on dataframe\n",
    "    temp = pd.concat([index.rename('index'), y.rename('true max'), max_value.rename('recovered_peak'), residuals_max.rename('peak residuals'), adjusted.rename('adjusted peak'), residuals_adj.rename('adjusted residuals')], axis = 1)\n",
    "    #set the index of the dataframe to the number of the events\n",
    "    temp = temp.set_index('index')\n",
    "\n",
    "\n",
    "    return temp\n",
    "\n",
    "\n",
    "amp_pico_ch3, amp_nano_ch3  = (OrderedDict([]) for i in range(2))\n",
    "#find the polynomial coefficents for the 2 GSPS and the 1 GSPS datasets--channel 3\n",
    "p_peak_p_ch3 = findpoly(great_ch3['max_vals'], filtered_pico_ch3, five_hundred_pico_ch3[str(0)].times.iloc[len(five_hundred_pico_ch3[str(0)])-1])\n",
    "p_peak_n_ch3 = findpoly(great_ch3['max_vals'], filtered_nano_ch3, one_nano_ch3[str(0)].times.iloc[len(one_nano_ch3[str(0)])-1])\n",
    "#again, we sort data based on the highest voltage threshold that is crossed\n",
    "for value in range(0, len(downsampled_thres)):\n",
    "    #find the organized dataframe\n",
    "    temp = organizer(thresholded_data_pico_ch3, downsampled_thres[value], great_ch3, filtered_pico_ch3, p_peak_p_ch3, five_hundred_pico_ch3[str(0)].times.iloc[len(five_hundred_pico_ch3[str(0)])-1])\n",
    "    print(len(temp))\n",
    "    #we don't want to save the dataframe if it has no data in it\n",
    "    if len(temp) > 0:\n",
    "        amp_pico_ch3[str(downsampled_thres[value])] = temp\n",
    "    \n",
    "    #find the organized dataframe\n",
    "    temp = organizer(thresholded_data_nano_ch3, downsampled_thres[value], great_ch3, filtered_nano_ch3, p_peak_n_ch3, one_nano_ch3[str(0)].times.iloc[len(one_nano_ch3[str(0)])-1])\n",
    "    #don't save the dataframe if it has no data in it\n",
    "    if len(temp) > 0:\n",
    "        amp_nano_ch3[str(downsampled_thres[value])] = temp\n",
    "        \n",
    "amp_pico_ch4, amp_nano_ch4  = (OrderedDict([]) for i in range(2))\n",
    "#find the polynomial coefficents for the 2 GSPS and the 1 GSPS datasets--channel 4\n",
    "p_peak_p_ch4 = findpoly(great_ch4['max_vals'], filtered_pico_ch4, five_hundred_pico_ch4[str(0)].times.iloc[len(five_hundred_pico_ch4[str(0)])-1])\n",
    "p_peak_n_ch4 = findpoly(great_ch4['max_vals'], filtered_nano_ch4, one_nano_ch4[str(0)].times.iloc[len(one_nano_ch4[str(0)])-1])\n",
    "#again, we sort data based on the highest voltage threshold that is crossed\n",
    "for value in range(0, len(downsampled_thres)):\n",
    "    #find the organized dataframe\n",
    "    temp = organizer(thresholded_data_pico_ch4, downsampled_thres[value], great_ch4, filtered_pico_ch4, p_peak_p_ch4, five_hundred_pico_ch4[str(0)].times.iloc[len(five_hundred_pico_ch4[str(0)])-1])\n",
    "    print(len(temp))\n",
    "    #we dont want to save the dataframe if it has no data in it\n",
    "    if len(temp) > 0:\n",
    "        amp_pico_ch4[str(downsampled_thres[value])] = temp\n",
    "    \n",
    "    #find the organized dataframe\n",
    "    temp = organizer(thresholded_data_nano_ch4, downsampled_thres[value], great_ch4, filtered_nano_ch4, p_peak_n_ch4, one_nano_ch4[str(0)].times.iloc[len(one_nano_ch4[str(0)])-1])\n",
    "    #we dont want to save the dataframe if it has no data in it\n",
    "    if len(temp) > 0:\n",
    "        amp_nano_ch4[str(downsampled_thres[value])] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I'm not going to comment the following cell as well because it does the same as above, but for the 10 GSPS data.  Because the data is finer sampled in time the order can generally be dropped for the polynomial from an order 4 to order 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findpoly_ori(y, flit, end):\n",
    "    max_value = pd.Series([])\n",
    "    last = pd.Series([])\n",
    "    a = 0\n",
    "    for event, values in flit.items():\n",
    "        max_value[a] = flit[str(event)]['Voltages'].max()\n",
    "        last[a] = flit[str(event)]['times'].loc[len(flit[str(event)])-1]\n",
    "        a += 1\n",
    "        \n",
    "    end = end - 10\n",
    "   \n",
    "    temp = pd.DataFrame([max_value.rename('recovered'), y.rename('maximum'), last.rename('last')])\n",
    "    temp = temp.T\n",
    "    temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "    temp = temp.query('maximum < 44')\n",
    "    temp = temp[temp['last'] >= end]\n",
    "    \n",
    "    z = np.polyfit(temp['recovered'], temp['maximum'], 1)\n",
    "    p_peak = np.poly1d(z)\n",
    "\n",
    "    \n",
    "    return p_peak\n",
    "\n",
    "def sorter_ori(diction, threshold, great, peak, p_peak, end):\n",
    "    y, index, max_value, adj, max_times, adj_time, x = (pd.Series([]) for i in range(7))\n",
    "    a = 0\n",
    "    for event, values in diction.items():\n",
    "        #the data is organized based on threshold\n",
    "        if diction[str(event)]['Voltages'].max() == threshold and peak[str(event)]['Voltages'].max() > 10**-14 and diction[str(event)]['times'].iloc[len(diction[str(event)])-1] >= end - 10:\n",
    "            y[a] = great['max_vals'].loc[int(event)]\n",
    "            index[a] = int(event)\n",
    "            max_value[a] = peak[str(event)]['Voltages'].max()\n",
    "            a += 1\n",
    "            \n",
    "    if len(max_value) != 0:        \n",
    "        adj = p_peak(max_value)\n",
    "        #plt.plot(max_value, p_peak(max_value), 'bo')\n",
    "        #plt.show()\n",
    "\n",
    "    return y, index, max_value, adj\n",
    "\n",
    "def organizer_ori(diction, threshold, great, peak, p_peak, end):\n",
    "\n",
    "    #find all the seperate series\n",
    "    y, index, max_value, adj = sorter_ori(diction, threshold, great, peak, p_peak, end)\n",
    "\n",
    "    #we need to find some residuals to see how well the algoritm worked\n",
    "    residuals_max = np.subtract(y, max_value)\n",
    "    \n",
    "    adjusted = pd.Series(adj)\n",
    "    residuals_adj = np.subtract(y, adj)\n",
    "    \n",
    "    \n",
    "    #concat all the series into on dataframe\n",
    "    temp = pd.concat([index.rename('index'), y.rename('true max'), max_value.rename('recovered_peak'), residuals_max.rename('peak residuals'), adjusted.rename('adjusted peak'), residuals_adj.rename('adjusted residuals')], axis = 1)\n",
    "    #set the index of the dataframe to the number of the events\n",
    "    temp = temp.set_index('index')\n",
    "\n",
    "\n",
    "    return temp\n",
    "\n",
    "#crap = OrderedDict()\n",
    "p_peak_ch3 = findpoly_ori(great_ch3['max_vals'], filtered_ori_ch3, ori_ch3[str(0)].times.iloc[len(ori_ch3[str(0)])-1])\n",
    "amp_ori_ch3 = OrderedDict()\n",
    "for value in range(0, len(downsampled_thres)):\n",
    "    #find the organized dataframe\n",
    "    temp = organizer_ori(thresholded_data_ch3, downsampled_thres[value], great_ch3, filtered_ori_ch3, p_peak_ch3, ori_ch3[str(0)].times.iloc[len(ori_ch3[str(0)])-1])\n",
    "    #print downsampled_thres[value], downsampled_thres[value + 1], len(temp)\n",
    "    print(len(temp))\n",
    "    if len(temp) > 0:\n",
    "        amp_ori_ch3[str(downsampled_thres[value])] = temp\n",
    "        \n",
    "p_peak_ch4 = findpoly_ori(great_ch4['max_vals'], filtered_ori_ch4, ori_ch4[str(0)].times.iloc[len(ori_ch4[str(0)])-1])\n",
    "amp_ori_ch4 = OrderedDict()\n",
    "for value in range(0, len(downsampled_thres)):\n",
    "    #find the organized dataframe\n",
    "    temp = organizer_ori(thresholded_data_ch4, downsampled_thres[value], great_ch4, filtered_ori_ch4, p_peak_ch4, ori_ch4[str(0)].times.iloc[len(ori_ch4[str(0)])-1])\n",
    "    #print downsampled_thres[value], downsampled_thres[value + 1], len(temp)\n",
    "    print(len(temp))\n",
    "    if len(temp) > 0:\n",
    "        amp_ori_ch4[str(downsampled_thres[value])] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following cell plots the relationship between the maximum filter response peak and the real maximum voltage of each event.  The plotted lines are the polynomials generated above.  Do you notice any trends about the data?  Why do you think there are these trends?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 15))\n",
    "ax = plt.subplot(311)\n",
    "\n",
    "max_vals = pd.Series([])\n",
    "recov = pd.Series([])\n",
    "a = 0\n",
    "for key in amp_ori_ch3:\n",
    "    for idx in amp_ori_ch3[str(key)].index:\n",
    "        ax.plot(amp_ori_ch3[str(key)]['recovered_peak'].loc[idx], amp_ori_ch3[str(key)]['true max'].loc[idx], 'ro')\n",
    "        max_vals[a] = amp_ori_ch3[str(key)]['true max'].loc[idx]\n",
    "        recov[a] = amp_ori_ch3[str(key)]['recovered_peak'].loc[idx]\n",
    "        a+=1\n",
    "\n",
    "#ax.xlabel('recovered peak')\n",
    "#ax.ylabel('actual peak')\n",
    "temp = pd.DataFrame([pd.Series(max_vals).rename('max'), pd.Series(recov).rename('recovered')])\n",
    "temp = temp.T\n",
    "temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "z = np.polyfit(temp['recovered'], temp['max'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(temp['recovered'], p(temp['recovered']), 'k-')\n",
    "ax.set_title('Peak Comparison Original Data-Channel 3')\n",
    "\n",
    "ax2 = plt.subplot(312)\n",
    "\n",
    "max_vals = pd.Series([])\n",
    "recov = pd.Series([])\n",
    "a = 0\n",
    "for key in amp_pico_ch3:\n",
    "    for idx in amp_pico_ch3[str(key)].index:\n",
    "        ax2.plot(amp_pico_ch3[str(key)]['recovered_peak'].loc[idx], amp_pico_ch3[str(key)]['true max'].loc[idx], 'ro')\n",
    "        max_vals[a] = amp_pico_ch3[str(key)]['true max'].loc[idx]\n",
    "        recov[a] = amp_pico_ch3[str(key)]['recovered_peak'].loc[idx]\n",
    "        a += 1\n",
    "\n",
    "temp = pd.DataFrame([pd.Series(max_vals).rename('max'), pd.Series(recov).rename('recovered')])\n",
    "temp = temp.T\n",
    "temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "z = np.polyfit(temp['recovered'], temp['max'], 4)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(temp['recovered'], p(temp['recovered']), 'k-')\n",
    "ax2.set_title('Peak Comparison 500ps Data-Channel 3')\n",
    "ax2.set_ylabel('Actual Peak')\n",
    "\n",
    "ax3 = plt.subplot(313)\n",
    "\n",
    "max_vals = pd.Series([])\n",
    "recov = pd.Series([])\n",
    "a = 0\n",
    "for key in amp_nano_ch3:\n",
    "    #print len(amp_nano[amp_nano.keys()[value]])\n",
    "    for idx in amp_nano_ch3[str(key)].index:\n",
    "        ax3.plot(amp_nano_ch3[str(key)]['recovered_peak'].loc[idx], amp_nano_ch3[str(key)]['true max'].loc[idx], 'ro')\n",
    "        max_vals[a] = amp_nano_ch3[str(key)]['true max'].loc[idx]\n",
    "        recov[a] = amp_nano_ch3[str(key)]['recovered_peak'].loc[idx]\n",
    "        a += 1\n",
    "        \n",
    "temp = pd.DataFrame([pd.Series(max_vals).rename('max'), pd.Series(recov).rename('recovered')])\n",
    "temp = temp.T\n",
    "temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "z = np.polyfit(temp['recovered'], temp['max'], 4)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(temp['recovered'], p(temp['recovered']), 'k-')\n",
    "ax3.set_title('Peak Comparison 1ns Data-Channel 3')\n",
    "ax3.set_xlabel('Response Peak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 15))\n",
    "ax = plt.subplot(311)\n",
    "\n",
    "max_vals = pd.Series([])\n",
    "recov = pd.Series([])\n",
    "a = 0\n",
    "for key in amp_ori_ch4:\n",
    "    for idx in amp_ori_ch4[str(key)].index:\n",
    "        ax.plot(amp_ori_ch4[str(key)]['recovered_peak'].loc[idx], amp_ori_ch4[str(key)]['true max'].loc[idx], 'ro')\n",
    "        max_vals[a] = amp_ori_ch4[str(key)]['true max'].loc[idx]\n",
    "        recov[a] = amp_ori_ch4[str(key)]['recovered_peak'].loc[idx]\n",
    "        a+=1\n",
    "\n",
    "#ax.xlabel('recovered peak')\n",
    "#ax.ylabel('actual peak')\n",
    "temp = pd.DataFrame([pd.Series(max_vals).rename('max'), pd.Series(recov).rename('recovered')])\n",
    "temp = temp.T\n",
    "temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "z = np.polyfit(temp['recovered'], temp['max'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(temp['recovered'], p(temp['recovered']), 'k-')\n",
    "ax.set_title('Peak Comparison Original Data-Channel 4')\n",
    "\n",
    "ax2 = plt.subplot(312)\n",
    "\n",
    "max_vals = pd.Series([])\n",
    "recov = pd.Series([])\n",
    "a = 0\n",
    "for key in amp_pico_ch4:\n",
    "    for idx in amp_pico_ch4[str(key)].index:\n",
    "        ax2.plot(amp_pico_ch4[str(key)]['recovered_peak'].loc[idx], amp_pico_ch4[str(key)]['true max'].loc[idx], 'ro')\n",
    "        max_vals[a] = amp_pico_ch4[str(key)]['true max'].loc[idx]\n",
    "        recov[a] = amp_pico_ch4[str(key)]['recovered_peak'].loc[idx]\n",
    "        a += 1\n",
    "\n",
    "temp = pd.DataFrame([pd.Series(max_vals).rename('max'), pd.Series(recov).rename('recovered')])\n",
    "temp = temp.T\n",
    "temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "z = np.polyfit(temp['recovered'], temp['max'], 4)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(temp['recovered'], p(temp['recovered']), 'k-')\n",
    "ax2.set_title('Peak Comparison 500ps Data-Channel 4')\n",
    "ax2.set_ylabel('Actual Peak')\n",
    "\n",
    "ax3 = plt.subplot(313)\n",
    "\n",
    "max_vals = pd.Series([])\n",
    "recov = pd.Series([])\n",
    "a = 0\n",
    "for key in amp_nano_ch4:\n",
    "    #print len(amp_nano[amp_nano.keys()[value]])\n",
    "    for idx in amp_nano_ch4[str(key)].index:\n",
    "        ax3.plot(amp_nano_ch4[str(key)]['recovered_peak'].loc[idx], amp_nano_ch4[str(key)]['true max'].loc[idx], 'ro')\n",
    "        max_vals[a] = amp_nano_ch4[str(key)]['true max'].loc[idx]\n",
    "        recov[a] = amp_nano_ch4[str(key)]['recovered_peak'].loc[idx]\n",
    "        a += 1\n",
    "        \n",
    "temp = pd.DataFrame([pd.Series(max_vals).rename('max'), pd.Series(recov).rename('recovered')])\n",
    "temp = temp.T\n",
    "temp = temp.sort_values(by=['recovered'], ascending = False)\n",
    "z = np.polyfit(temp['recovered'], temp['max'], 4)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(temp['recovered'], p(temp['recovered']), 'k-')\n",
    "ax3.set_title('Peak Comparison 1ns Data-Channel 4')\n",
    "ax3.set_xlabel('Response Peak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now lets look at how well our algorithm performed using these polynomial fit relationships.  Each graph below will be a histogram.  The top row will be voltage residual (ie. estimated voltage - actual voltage) divided by each threshold, the second row will be for the entire data set.  The title of the graph indicates which sampling rate each graph is for.  Again, do you notice any trends?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "len_subplots = len(range(0, len(amp_pico_ch3.keys())))\n",
    "fig = plt.figure(figsize = (35, 40))\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.figtext(0.5,0.90,\"Event Peak Residuals: 500 ps Sampling-Channel 3\", va=\"center\", ha=\"center\", size=20)\n",
    "\n",
    "value = 0\n",
    "for key in amp_pico_ch3:\n",
    "    ax = plt.subplot(4, len_subplots, value+1)\n",
    "    temp = amp_pico_ch3[str(key)]['adjusted residuals']\n",
    "    ax.hist(np.array(temp), bins = 20, normed = 1, color = 'cornflowerblue')\n",
    "    mean, std = norm.fit(temp)\n",
    "    #print 'downsampled thres: {} \\t mean: {}'.format(amp_pico.keys()[value], mean)\n",
    "    xmin, xmax = temp.min(), temp.max()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    ax.plot(x, y, color = 'black', label = 'stdev: '+str(round(std,5)), lw = 2)\n",
    "    ax.legend(fontsize = 16)\n",
    "    ax.set_title(round(float(key), 3), fontsize = 20)\n",
    "    value += 1\n",
    "\n",
    "\n",
    "axes2 = plt.subplot(412)\n",
    "axes2.set_title(\"Recovered Amplitude Residuals\", fontsize = 20)\n",
    "peak_residuals = amp_pico_ch3[str(list(amp_pico_ch3.keys())[0])]['adjusted residuals']\n",
    "for key in amp_pico_ch3:\n",
    "        peak_residuals = pd.concat([peak_residuals, amp_pico_ch3[str(key)]['adjusted residuals']])\n",
    "mean, std = norm.fit(peak_residuals)\n",
    "xmin, xmax = peak_residuals.min(), peak_residuals.max()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = norm.pdf(x, mean, std)\n",
    "axes2.hist(peak_residuals, 50, normed = 1, color = 'cornflowerblue')\n",
    "axes2.plot(x, y, color = 'black', ls = '-', label = 'stdev: '+str(round(std,5)))\n",
    "axes2.legend(fontsize = 16)\n",
    "axes2.set_xlabel('Actual Peak - Adjusted Peak', fontsize = 16)\n",
    "\n",
    "len_subplots = len(range(0, len(amp_pico_ch4.keys())))\n",
    "plt.figtext(0.5,0.50,\"Event Peak Residuals: 500 ps Sampling-Channel 4\", va=\"center\", ha=\"center\", size=20)\n",
    "\n",
    "value = 0\n",
    "for key in amp_pico_ch4:\n",
    "    ax = plt.subplot(4, len_subplots, value+13)\n",
    "    temp = amp_pico_ch4[str(key)]['adjusted residuals']\n",
    "    ax.hist(np.array(temp), bins = 20, normed = 1, color = 'hotpink')\n",
    "    mean, std = norm.fit(temp)\n",
    "    #print 'downsampled thres: {} \\t mean: {}'.format(amp_pico.keys()[value], mean)\n",
    "    xmin, xmax = temp.min(), temp.max()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    ax.plot(x, y, color = 'black', label = 'stdev: '+str(round(std,5)), lw = 2)\n",
    "    ax.legend(fontsize = 16)\n",
    "    ax.set_title(round(float(key), 3), fontsize = 20)\n",
    "    value += 1\n",
    "\n",
    "\n",
    "axes2 = plt.subplot(414)\n",
    "axes2.set_title(\"Recovered Amplitude Residuals\", fontsize = 20)\n",
    "peak_residuals = amp_pico_ch4[str(list(amp_pico_ch4.keys())[0])]['adjusted residuals']\n",
    "for key in amp_pico_ch4:\n",
    "        peak_residuals = pd.concat([peak_residuals, amp_pico_ch4[str(key)]['adjusted residuals']])\n",
    "mean, std = norm.fit(peak_residuals)\n",
    "xmin, xmax = peak_residuals.min(), peak_residuals.max()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = norm.pdf(x, mean, std)\n",
    "axes2.hist(peak_residuals, 50, normed = 1, color = 'hotpink')\n",
    "axes2.plot(x, y, color = 'black', ls = '-', label = 'stdev: '+str(round(std,5)))\n",
    "axes2.legend(fontsize = 16)\n",
    "axes2.set_xlabel('Actual Peak - Adjusted Peak', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "len_subplots = len(range(0, len(amp_nano_ch3.keys())))\n",
    "fig = plt.figure(figsize = (35, 40))\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.figtext(0.5,0.90,\"Event Peak Residuals: 1 ns Sampling-Channel 3\", va=\"center\", ha=\"center\", size=20)\n",
    "\n",
    "value = 0\n",
    "for key in amp_nano_ch3:\n",
    "    ax = plt.subplot(4, len_subplots, value+1)\n",
    "    temp = amp_nano_ch3[str(key)]['adjusted residuals']\n",
    "    ax.hist(np.array(temp), bins = 20, normed = 1, color = 'cornflowerblue')\n",
    "    mean, std = norm.fit(temp)\n",
    "    #print 'downsampled thres: {} \\t mean: {}'.format(amp_pico.keys()[value], mean)\n",
    "    xmin, xmax = temp.min(), temp.max()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    ax.plot(x, y, color = 'black', label = 'stdev: '+str(round(std,5)), lw = 2)\n",
    "    ax.legend(fontsize = 16)\n",
    "    ax.set_title(round(float(key), 3), fontsize = 20)\n",
    "    value += 1\n",
    "\n",
    "\n",
    "axes2 = plt.subplot(412)\n",
    "axes2.set_title(\"Recovered Amplitude Residuals\", fontsize = 20)\n",
    "peak_residuals = amp_nano_ch3[str(list(amp_nano_ch3.keys())[0])]['adjusted residuals']\n",
    "for key in amp_nano_ch3:\n",
    "        peak_residuals = pd.concat([peak_residuals, amp_nano_ch3[str(key)]['adjusted residuals']])\n",
    "mean, std = norm.fit(peak_residuals)\n",
    "xmin, xmax = peak_residuals.min(), peak_residuals.max()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = norm.pdf(x, mean, std)\n",
    "axes2.hist(peak_residuals, 50, normed = 1, color = 'cornflowerblue')\n",
    "axes2.plot(x, y, color = 'black', ls = '-', label = 'stdev: '+str(round(std,5)))\n",
    "axes2.legend(fontsize = 16)\n",
    "axes2.set_xlabel('Actual Peak - Adjusted Peak', fontsize = 16)\n",
    "\n",
    "len_subplots = len(range(0, len(amp_nano_ch4.keys())))\n",
    "plt.figtext(0.5,0.50,\"Event Peak Residuals: 1 ns Sampling-Channel 4\", va=\"center\", ha=\"center\", size=20)\n",
    "\n",
    "value = 0\n",
    "for key in amp_nano_ch4:\n",
    "    ax = plt.subplot(4, len_subplots, value+13)\n",
    "    temp = amp_nano_ch4[str(key)]['adjusted residuals']\n",
    "    ax.hist(np.array(temp), bins = 20, normed = 1, color = 'hotpink')\n",
    "    mean, std = norm.fit(temp)\n",
    "    #print 'downsampled thres: {} \\t mean: {}'.format(amp_pico.keys()[value], mean)\n",
    "    xmin, xmax = temp.min(), temp.max()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    ax.plot(x, y, color = 'black', label = 'stdev: '+str(round(std,5)), lw = 2)\n",
    "    ax.legend(fontsize = 16)\n",
    "    ax.set_title(round(float(key), 3), fontsize = 20)\n",
    "    value += 1\n",
    "\n",
    "\n",
    "axes2 = plt.subplot(414)\n",
    "axes2.set_title(\"Recovered Amplitude Residuals\", fontsize = 20)\n",
    "peak_residuals = amp_nano_ch4[str(list(amp_nano_ch4.keys())[0])]['adjusted residuals']\n",
    "for key in amp_nano_ch4:\n",
    "        peak_residuals = pd.concat([peak_residuals, amp_nano_ch4[str(key)]['adjusted residuals']])\n",
    "mean, std = norm.fit(peak_residuals)\n",
    "xmin, xmax = peak_residuals.min(), peak_residuals.max()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = norm.pdf(x, mean, std)\n",
    "axes2.hist(peak_residuals, 50, normed = 1, color = 'hotpink')\n",
    "axes2.plot(x, y, color = 'black', ls = '-', label = 'stdev: '+str(round(std,5)))\n",
    "axes2.legend(fontsize = 16)\n",
    "axes2.set_xlabel('Actual Peak - Adjusted Peak', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "len_subplots = len(range(0, len(amp_ori_ch3.keys())))\n",
    "fig = plt.figure(figsize = (35, 40))\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.figtext(0.5,0.90,\"Event Peak Residuals: 10Gsps Timing and Original Voltage-Channel 3\", va=\"center\", ha=\"center\", size=20)\n",
    "\n",
    "value = 0\n",
    "for key in amp_ori_ch3:\n",
    "    ax = plt.subplot(4, len_subplots, value+1)\n",
    "    temp = amp_ori_ch3[str(key)]['adjusted residuals']\n",
    "    ax.hist(np.array(temp), bins = 20, normed = 1, color = 'cornflowerblue')\n",
    "    mean, std = norm.fit(temp)\n",
    "    #print 'downsampled thres: {} \\t mean: {}'.format(amp_pico.keys()[value], mean)\n",
    "    xmin, xmax = temp.min(), temp.max()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    ax.plot(x, y, color = 'black', label = 'stdev: '+str(round(std,5)), lw = 2)\n",
    "    ax.legend(fontsize = 16)\n",
    "    ax.set_title(round(float(key), 3), fontsize = 20)\n",
    "    value += 1\n",
    "\n",
    "\n",
    "axes2 = plt.subplot(412)\n",
    "axes2.set_title(\"Recovered Amplitude Residuals\", fontsize = 20)\n",
    "peak_residuals = amp_ori_ch3[str(list(amp_ori_ch3.keys())[0])]['adjusted residuals']\n",
    "for key in amp_ori_ch3:\n",
    "        peak_residuals = pd.concat([peak_residuals, amp_ori_ch3[str(key)]['adjusted residuals']])\n",
    "mean, std = norm.fit(peak_residuals)\n",
    "xmin, xmax = peak_residuals.min(), peak_residuals.max()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = norm.pdf(x, mean, std)\n",
    "axes2.hist(peak_residuals, 50, normed = 1, color = 'cornflowerblue')\n",
    "axes2.plot(x, y, color = 'black', ls = '-', label = 'stdev: '+str(round(std,5)))\n",
    "axes2.legend(fontsize = 16)\n",
    "axes2.set_xlabel('Actual Peak - Adjusted Peak', fontsize = 16)\n",
    "\n",
    "len_subplots = len(range(0, len(amp_ori_ch4.keys())))\n",
    "plt.figtext(0.5,0.50,\"Event Peak Residuals: 10Gsps Timing and Original Voltage-Channel 4\", va=\"center\", ha=\"center\", size=20)\n",
    "\n",
    "value = 0\n",
    "for key in amp_ori_ch4:\n",
    "    ax = plt.subplot(4, len_subplots, value+13)\n",
    "    temp = amp_ori_ch4[str(key)]['adjusted residuals']\n",
    "    ax.hist(np.array(temp), bins = 20, normed = 1, color = 'hotpink')\n",
    "    mean, std = norm.fit(temp)\n",
    "    #print 'downsampled thres: {} \\t mean: {}'.format(amp_pico.keys()[value], mean)\n",
    "    xmin, xmax = temp.min(), temp.max()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    ax.plot(x, y, color = 'black', label = 'stdev: '+str(round(std,5)), lw = 2)\n",
    "    ax.legend(fontsize = 16)\n",
    "    ax.set_title(round(float(key), 3), fontsize = 20)\n",
    "    value += 1\n",
    "\n",
    "\n",
    "axes2 = plt.subplot(414)\n",
    "axes2.set_title(\"Recovered Amplitude Residuals\", fontsize = 20)\n",
    "peak_residuals = amp_ori_ch4[str(list(amp_ori_ch4.keys())[0])]['adjusted residuals']\n",
    "for key in amp_ori_ch4:\n",
    "        peak_residuals = pd.concat([peak_residuals, amp_ori_ch4[str(key)]['adjusted residuals']])\n",
    "mean, std = norm.fit(peak_residuals)\n",
    "xmin, xmax = peak_residuals.min(), peak_residuals.max()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "y = norm.pdf(x, mean, std)\n",
    "axes2.hist(peak_residuals, 50, normed = 1, color = 'hotpink')\n",
    "axes2.plot(x, y, color = 'black', ls = '-', label = 'stdev: '+str(round(std,5)))\n",
    "axes2.legend(fontsize = 16)\n",
    "axes2.set_xlabel('Actual Peak - Adjusted Peak', fontsize = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
